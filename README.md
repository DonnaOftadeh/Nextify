# ğŸ§  Nextify â€” Modular, Multi-Agent Product Strategy Assistant

**Nextify** is your product management brain â€” upgraded. Itâ€™s a modular, multi-agent system built to assist (and often outperform) traditional PM workflows. Designed during the Google GenAI Capstone, Nextify started with prompt evaluation but now powers a full AI-driven feature and roadmap engine.

It intelligently generates product features, ICE scores, OKRs, strategic roadmaps, competitive insights, and even multi-format storytelling â€” while grounding recommendations in prompts, agents, and (soon) embeddings.

---

## ğŸ” What Does It Do?

- âœ… Evaluate prompt strategies across LLM runs and human feedback
- ğŸ” Compare Zero-Shot, Few-Shot, Chain-of-Thought, ReAct, and more
- ğŸ§  Generate structured product strategy outputs:
  - Mission & Vision
  - Feature Prioritization (ICE)
  - Strategic Roadmaps (short/mid/long-term)
  - SMART OKRs
  - Next-level innovation
- ğŸ§© Modular agent design â€” agents for feature ideation, roadmap planning, OKR writing, feedback parsing, and competitive analysis
- ğŸ“¦ Multi-format output generation (JSON, text, blog post, video outline)

---

## ğŸ“Š Live Interactive Dashboard

Deployed on [Render.com](https://render.com), the dashboard lets you:

- Filter by `Company`, `Strategy`, `Prompt Tag`, or `Run`
- View evaluation tables with LLM & Human scores
- See full LLM outputs (in Markdown or JSON mode)
- Track score trends across runs
- Expand feedback and lessons per prompt
- Explore placeholder tabs for:
  - ğŸ§¬ Embedding-based similarity (coming)
  - ğŸ¤– Agent evaluations (multi-agent chain visibility)

### â¡ï¸ [View the Live Dashboard](https://nextify-dashboard.onrender.com)

---

## ğŸ“ Project Structure

```
productify-next/
â”œâ”€â”€ app/
â”‚   â””â”€â”€ app.py                  # Streamlit dashboard logic
â”œâ”€â”€ data/
â”‚   â””â”€â”€ all_experiment_view.csv  # Evaluation data across runs
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ prompt_outputs.json     # Raw prompt + response archives
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ§ª Prompting Framework

Supports side-by-side comparison of:
- Zero-shot
- Few-shot
- Chain-of-Thought (CoT)
- Tree-of-Thought
- ReAct
- Combined approaches

Each strategy logs:
- Prompt content
- LLM-generated output
- Self-evaluation (LLM score)
- Human evaluation
- Feedback + lessons

---

## ğŸ§  Future Modules

| Component           | Status     | Description |
|---------------------|------------|-------------|
| Embedding Explorer  | ğŸ› ï¸ in progress | Document-level product understanding |
| Multi-Agent Orchestration | ğŸ› ï¸ prototyping | Role-based agent chains for PM tasks |
| Fine-tuning Stub    | âœ… planned  | Dataset formatter for continued LLM training |
| Real-Time Interface | ğŸ§ª demo     | Goal: Reactive prompt + feedback loop |

---

## ğŸ› ï¸ Local Run

```bash
pip install -r requirements.txt
streamlit run app/app.py
```
Here you can find the dashboard link to streamlit dashboard:
https://nextify-dashboard-100.streamlit.app/
---

## ğŸ“š License

MIT â€” feel free to fork, remix, and evolve the PM workflow of the future.
